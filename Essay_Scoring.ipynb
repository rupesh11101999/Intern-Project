{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Essay Scoring.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnkeaifSGWIa",
        "colab_type": "text"
      },
      "source": [
        "# **Automated Essay Scoring using Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RtUXyNX6fKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Have to use java8 for language_check\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV6hCtmIwvBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install language-check\n",
        "!pip install skll\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BDbf59WJnSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder,StandardScaler\n",
        "from sklearn.model_selection import cross_val_score,KFold,train_test_split\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import language_check\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "from skll.metrics import kappa\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Input, LSTM, Embedding, Bidirectional, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "import en_core_web_sm\n",
        "\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZbBq2SR9rCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_quadratic_weighted_kappa(kappas, weights=None):\n",
        "    \"\"\"\n",
        "    Calculates the mean of the quadratic\n",
        "    weighted kappas after applying Fisher's r-to-z transform, which is\n",
        "    approximately a variance-stabilizing transformation.  This\n",
        "    transformation is undefined if one of the kappas is 1.0, so all kappa\n",
        "    values are capped in the range (-0.999, 0.999).  The reverse\n",
        "    transformation is then applied before returning the result.\n",
        "    \n",
        "    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n",
        "    kappa values\n",
        "    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n",
        "    of weights that is the same size as kappas.  Weights are applied in the\n",
        "    z-space\n",
        "    \"\"\"\n",
        "    kappas = np.array(kappas, dtype=float)\n",
        "    if weights is None:\n",
        "        weights = np.ones(np.shape(kappas))\n",
        "    else:\n",
        "        weights = weights / np.mean(weights)\n",
        "\n",
        "    # ensure that kappas are in the range [-.999, .999]\n",
        "    kappas = np.array([min(x, .999) for x in kappas])\n",
        "    kappas = np.array([max(x, -.999) for x in kappas])\n",
        "    \n",
        "    z = 0.5 * np.log( (1+kappas)/(1-kappas) ) * weights\n",
        "    z = np.mean(z)\n",
        "    kappa = (np.exp(2*z)-1) / (np.exp(2*z)+1)\n",
        "    return kappa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfk5dk3JnS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using language tool to correct most spelling and grammatical errors.\n",
        "\n",
        "def correct_language(df):\n",
        "    tool = language_check.LanguageTool('en-US')\n",
        "    df['matches'] = df['essay'].apply(lambda txt:tool.check(txt))\n",
        "    df['corrections'] = df.apply(lambda l:len(l['matches']),axis=1)\n",
        "    df['corrected'] = df.apply(lambda l:language_check.correct(l['essay'],l['matches']),axis=1)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqToP2S-9lKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read essays from training_set\n",
        "training_set = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data asap aes/training_set_rel3.tsv',sep='\\t',encoding=\"ISO-8859-1\")\\\n",
        "            .rename(columns={'essay_set': 'topic', 'domain1_score': 'target_score', 'domain2_score': 'topic2_target'})\n",
        "\n",
        "# Count characters and words for each essay\n",
        "training_set['word_count'] = training_set['essay'].str.strip().str.split().str.len()\n",
        "\n",
        "# apply spelling and grammar corrections\n",
        "training_set = correct_language(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJmC6l19st7",
        "colab_type": "text"
      },
      "source": [
        "**NLP with Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82nLeaD-r_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sents = []\n",
        "tokens = []\n",
        "lemma = []\n",
        "pos = []\n",
        "ner = []\n",
        "\n",
        "stop_words = set(STOP_WORDS)\n",
        "stop_words.update(punctuation)\n",
        "\n",
        "for essay in nlp.pipe(training_set['corrected'], batch_size = 100, n_threads = 3):\n",
        "    if essay.is_parsed:\n",
        "        tokens.append([e.text for e in essay])\n",
        "        sents.append([sent.string.strip() for sent in essay.sents])\n",
        "        pos.append([e.pos_ for e in essay])\n",
        "        ner.append([e.text for e in essay.ents])\n",
        "        lemma.append([n.lemma_ for n in essay])\n",
        "    else:\n",
        "        # We want to make sure that the lists of parsed results have the\n",
        "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
        "        tokens.append(None)\n",
        "        lemma.append(None)\n",
        "        pos.append(None)\n",
        "        sents.append(None)\n",
        "        ner.append(None)\n",
        "\n",
        "training_set['tokens'] = tokens\n",
        "training_set['lemma'] = lemma\n",
        "training_set['pos'] = pos\n",
        "training_set['sents'] = sents\n",
        "training_set['ner'] = ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQp4bJjz_QBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each topic, essay with highest available target_score has been chosen.\n",
        "# All other essays will be compared to these.\n",
        "\n",
        "reference_essays = {1: 161, 2: 3022, 3: 5263, 4: 5341, 5: 7209, 6: 8896, 7: 11796, 8: 12340} # topic: essay_id\n",
        "\n",
        "references = {}\n",
        "\n",
        "for topic, index in reference_essays.items():\n",
        "    references[topic] = nlp(training_set.iloc[index]['essay'])\n",
        "\n",
        "# generate document similarity for each essay compared to topic reference\n",
        "training_set['similarity'] = training_set.apply(lambda row: nlp(row['essay']).similarity(references[row['topic']]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15xo_K8sJnS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Various other features are counted.\n",
        "\n",
        "training_set['token_count'] = training_set.apply(lambda x: len(x['tokens']), axis=1)\n",
        "training_set['unique_token_count'] = training_set.apply(lambda x: len(set(x['tokens'])), axis=1)\n",
        "training_set['nostop_count'] = training_set \\\n",
        "            .apply(lambda x: len([token for token in x['tokens'] if token not in stop_words]), axis=1)\n",
        "training_set['sent_count'] = training_set.apply(lambda x: len(x['sents']), axis=1)\n",
        "training_set['ner_count'] = training_set.apply(lambda x: len(x['ner']), axis=1)\n",
        "training_set['comma'] = training_set.apply(lambda x: x['corrected'].count(','), axis=1)\n",
        "training_set['question'] = training_set.apply(lambda x: x['corrected'].count('?'), axis=1)\n",
        "training_set['exclamation'] = training_set.apply(lambda x: x['corrected'].count('!'), axis=1)\n",
        "training_set['quotation'] = training_set.apply(lambda x: x['corrected'].count('\"') + x['corrected'].count(\"'\"), axis=1)\n",
        "training_set['organization'] = training_set.apply(lambda x: x['corrected'].count(r'@ORGANIZATION'), axis=1)\n",
        "training_set['caps'] = training_set.apply(lambda x: x['corrected'].count(r'@CAPS'), axis=1)\n",
        "training_set['person'] = training_set.apply(lambda x: x['corrected'].count(r'@PERSON'), axis=1)\n",
        "training_set['location'] = training_set.apply(lambda x: x['corrected'].count(r'@LOCATION'), axis=1)\n",
        "training_set['money'] = training_set.apply(lambda x: x['corrected'].count(r'@MONEY'), axis=1)\n",
        "training_set['time'] = training_set.apply(lambda x: x['corrected'].count(r'@TIME'), axis=1)\n",
        "training_set['date'] = training_set.apply(lambda x: x['corrected'].count(r'@DATE'), axis=1)\n",
        "training_set['percent'] = training_set.apply(lambda x: x['corrected'].count(r'@PERCENT'), axis=1)\n",
        "training_set['noun'] = training_set.apply(lambda x: x['pos'].count('NOUN'), axis=1)\n",
        "training_set['adj'] = training_set.apply(lambda x: x['pos'].count('ADJ'), axis=1)\n",
        "training_set['pron'] = training_set.apply(lambda x: x['pos'].count('PRON'), axis=1)\n",
        "training_set['verb'] = training_set.apply(lambda x: x['pos'].count('VERB'), axis=1)\n",
        "training_set['noun'] = training_set.apply(lambda x: x['pos'].count('NOUN'), axis=1)\n",
        "training_set['cconj'] = training_set.apply(lambda x: x['pos'].count('CCONJ'), axis=1)\n",
        "training_set['adv'] = training_set.apply(lambda x: x['pos'].count('ADV'), axis=1)\n",
        "training_set['det'] = training_set.apply(lambda x: x['pos'].count('DET'), axis=1)\n",
        "training_set['propn'] = training_set.apply(lambda x: x['pos'].count('PROPN'), axis=1)\n",
        "training_set['num'] = training_set.apply(lambda x: x['pos'].count('NUM'), axis=1)\n",
        "training_set['part'] = training_set.apply(lambda x: x['pos'].count('PART'), axis=1)\n",
        "training_set['intj'] = training_set.apply(lambda x: x['pos'].count('INTJ'), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1BAtyBH_hqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwUGQRx2Tn3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set.to_pickle('/content/drive/My Drive/Colab Notebooks/Data asap aes/training_features.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6pw91cUbQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/Data asap aes/training_features.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37wYR0xrJnTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read essays from validation and test sets\n",
        "\n",
        "valid_set = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data asap aes/valid_set.tsv',sep='\\t',encoding=\"ISO-8859-1\")\\\n",
        "            .rename(columns={'essay_set':'topic'})\n",
        "\n",
        "test_set = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data asap aes/test_set.tsv',sep='\\t',encoding=\"ISO-8859-1\")\\\n",
        "          .rename(columns={'essay_set':'topic'})\n",
        "\n",
        "combo_set = pd.concat([valid_set,test_set],sort = False)\n",
        "\n",
        "# apply spelling and grammar corrections\n",
        "combo_set = correct_language(combo_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsmBjgChJnTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16834160-57bc-474c-d9b0-7c1e5326ab65"
      },
      "source": [
        "combo_set = pd.concat([combo_set,training_set],sort = False)\n",
        "combo_set.to_pickle('/content/drive/My Drive/Colab Notebooks/Data asap aes/combo_set.pkl')\n",
        "len(combo_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBBnvhNZnWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combo_set = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/Data asap aes/combo_set.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsT6HZKkBULC",
        "colab_type": "text"
      },
      "source": [
        "**Generate word embeddings with Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr0ohsKJJnTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean training_set essays before feeding them to the Word2Vec model.\n",
        "\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Function for cleaning text by removing personal pronouns, stopwords, and puncuation\n",
        "def cleanup_essays(essays,logging = False):\n",
        "    texts = []\n",
        "    counter = 1\n",
        "    for essay in essays.corrected:\n",
        "        counter += 1\n",
        "        essay = nlp(essay,disable=['parser','ner'])\n",
        "        tokens = [tok.lemma_.lower().strip() for tok in essay if tok.lemma_ != '-PRON-']\n",
        "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
        "        tokens = ' '.join(tokens)\n",
        "        texts.append(tokens)\n",
        "    return pd.Series(texts) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66kJ2UX8JnTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cleaned = cleanup_essays(training_set,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCwlGGtxJnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to preprocess text for a word2vec model\n",
        "def cleanup_essays_word2vec(essays, logging=False):\n",
        "    sentences = []\n",
        "    counter = 1\n",
        "    for essay in essays:\n",
        "        essay = nlp(essay,disable = ['tagger'])\n",
        "        essay = \" \".join([tok.lemma_.lower() for tok in essay])\n",
        "        essay = re.split(\"[\\.?!;] \", essay)\n",
        "        essay = [re.sub(\"[\\.,;:!?]\", \"\", sent) for sent in essay]\n",
        "        essay = [sent.split() for sent in essay]\n",
        "        sentences += essay\n",
        "        counter += 1\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVl3TtVIJnT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_word2vec = cleanup_essays_word2vec(combo_set['corrected'],logging = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP93Nq7DJnUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e36dc72b-9c97-4a67-e4a0-2ab8469d7641"
      },
      "source": [
        "text_dim = 300\n",
        "wordvec_model = Word2Vec(cleaned_word2vec, size=text_dim, window=5, min_count=3, workers=4, sg=1)\n",
        "wordvec_model.save('wordvec_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYSI43TZJnUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create averaged word vectors given a cleaned text.\n",
        "def create_average_vec(essay):\n",
        "    average = np.zeros((text_dim,), dtype='float32')\n",
        "    num_words = 0.\n",
        "    for word in essay.split():\n",
        "        if word in wordvec_model.wv.vocab:\n",
        "            average = np.add(average, wordvec_model.wv[word])\n",
        "            num_words += 1.\n",
        "    if num_words != 0.:\n",
        "        average = np.divide(average, num_words)\n",
        "    return average"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0CyXSHtJnUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9096c10-a6b4-474c-b415-b59ede5a42b9"
      },
      "source": [
        "# Create word vectors\n",
        "cleaned_vec = np.zeros((training_set.shape[0], text_dim), dtype=\"float32\")  \n",
        "for i in range(len(train_cleaned)):\n",
        "    cleaned_vec[i] = create_average_vec(train_cleaned[i])\n",
        "\n",
        "print(\"Word vectors for all essays in the training data set are of shape:\", cleaned_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word vectors for all essays in the training data set are of shape: (12976, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT5ubOqdED7f",
        "colab_type": "text"
      },
      "source": [
        "**Neural Network Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlla2CWUJnUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a87077d0-c683-45cf-d36e-10481cade3eb"
      },
      "source": [
        "feature_list = [\n",
        "                'word_count',\n",
        "                'corrections',\n",
        "                'similarity',\n",
        "                'token_count',\n",
        "                'unique_token_count',\n",
        "                'nostop_count',\n",
        "                'sent_count',\n",
        "                'ner_count',\n",
        "                'comma',\n",
        "                'question',\n",
        "                'exclamation',\n",
        "                'quotation',\n",
        "                'organization',\n",
        "                'caps',\n",
        "                'person',\n",
        "                'location',\n",
        "                'money',\n",
        "                'time',\n",
        "                'date',\n",
        "                'percent',\n",
        "                'noun',\n",
        "                'adj',\n",
        "                'pron',\n",
        "                'verb',\n",
        "                'cconj',\n",
        "                'adv',\n",
        "                'det',\n",
        "                'propn',\n",
        "                'num',\n",
        "                'part',\n",
        "                'intj'\n",
        "                ]\n",
        "\n",
        "additional_features = training_set[feature_list]\n",
        "\n",
        "stdscaler = StandardScaler()\n",
        "additional_features = stdscaler.fit_transform(additional_features)\n",
        "additional_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12976, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RtJeJ2nJnUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "004e96d2-ca9a-4be1-f5dd-bc372824c1e4"
      },
      "source": [
        "# Combine topic number, target score, additional features and cleaned word vectors\n",
        "all_data = pd.concat([training_set[['topic','target_score']],pd.DataFrame(additional_features), pd.DataFrame(cleaned_vec)], axis=1)\n",
        "all_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12976, 333)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW04nOiAJnUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "66727d49-b461-4b54-9a3b-21933f3e177d"
      },
      "source": [
        "# Build model\n",
        "output_dim = 1\n",
        "input_dim = all_data.shape[1] - 2\n",
        "model = None\n",
        "dropout = 0.2\n",
        "model = Sequential()\n",
        "model.add(Dense(14,activation='relu',kernel_initializer='he_normal',input_dim=input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(output_dim))\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(lr=0.0001,beta_1=0.9,beta_2=0.999,epsilon=10e-8,decay=0.0,amsgrad=False)\n",
        "model.compile(optimizer=adam,loss='mse',metrics=['mae','mse'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 14)                4648      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 14)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 15        \n",
            "=================================================================\n",
            "Total params: 4,663\n",
            "Trainable params: 4,663\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CaUIbnGJnUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77a64b2d-b433-47b8-c001-263311593901"
      },
      "source": [
        "# Run each topic individually through neural network\n",
        "\n",
        "kappa_list = []\n",
        "weights = []\n",
        "epochs = 100\n",
        "\n",
        "for topic in range(1,9):\n",
        "    X = all_data[all_data.topic==topic].drop(['topic','target_score'],axis=1)\n",
        "    y = all_data[all_data.topic==topic].target_score.to_frame()\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=26)\n",
        "    estimator = model.fit(X_train,y_train,epochs=epochs,batch_size=15,verbose=0)\n",
        "\n",
        "    # get predictions\n",
        "    y_pred = pd.DataFrame(model.predict(X_test).reshape(-1))\n",
        "\n",
        "    # get topic kappa score\n",
        "    kappa_list.append(kappa(y_test.values,y_pred.round(0).astype(int).values,weights='quadratic'))\n",
        "\n",
        "    # get weights\n",
        "    weights.append(y_test.shape[0]/all_data.shape[0])   \n",
        "\n",
        "# get weighted average kappa\n",
        "qwk = mean_quadratic_weighted_kappa(kappa_list, weights=1) # weights)\n",
        "print(qwk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7332403066846499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQF9Ic_4JnUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9eb0de66-e5a3-4e99-9c9e-33a9d6684660"
      },
      "source": [
        "# Cross-validation\n",
        "\n",
        "kappa_dict = {}\n",
        "for topic in range(1,9):\n",
        "    model = None\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(14,activation='relu',kernel_initializer='he_normal',input_dim=input_dim))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss = 'mean_squared_error',optimizer = adam)\n",
        "\n",
        "    X = all_data[all_data.topic == topic].drop(['topic','target_score'],axis=1)\n",
        "    y = all_data[all_data.topic == topic].target_score.to_frame()\n",
        "    kf = KFold(n_splits =5,random_state = 26)\n",
        "    kappa_list = []\n",
        "    for train,test in kf.split(X):\n",
        "        X_train,X_test = X.iloc[train],X.iloc[test]\n",
        "        y_train,y_test = y.iloc[train],y.iloc[test]\n",
        "        model.fit(X_train,y_train,epochs=200,batch_size=15,verbose=0)\n",
        "        y_pred = pd.DataFrame(model.predict(X_test).reshape(-1))\n",
        "        kappa_list.append(kappa(y_pred.round(0).astype(int).values,\n",
        "                               y.iloc[test].values,\n",
        "                               weights='quadratic'))\n",
        "    print(\"Kappa for topic\", topic, \": {:.3f}%\".format(np.mean(kappa_list)))\n",
        "    kappa_dict[topic] = np.mean(kappa_list)\n",
        "    \n",
        "mqwk = mean_quadratic_weighted_kappa(list(kappa_dict.values()), weights=1) # weights)\n",
        "print(mqwk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Kappa for topic 1 : 0.818%\n",
            "Kappa for topic 2 : 0.711%\n",
            "Kappa for topic 3 : 0.712%\n",
            "Kappa for topic 4 : 0.775%\n",
            "Kappa for topic 5 : 0.819%\n",
            "Kappa for topic 6 : 0.811%\n",
            "Kappa for topic 7 : 0.779%\n",
            "Kappa for topic 8 : 0.629%\n",
            "0.7631138266404067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfLrzV7Ip64s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}